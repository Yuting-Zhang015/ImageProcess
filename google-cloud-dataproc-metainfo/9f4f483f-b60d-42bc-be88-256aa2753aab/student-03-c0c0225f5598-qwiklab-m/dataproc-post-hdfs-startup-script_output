+ source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
++ [[ /opt/conda/default/bin:/opt/conda/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ DATAPROC_DIR=/usr/local/share/google/dataproc
++ DATAPROC_TMP_DIR=/tmp/dataproc
++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
++ INSTALL_GCS_CONNECTOR=1
++ INSTALL_BIGQUERY_CONNECTOR=1
++ ENABLE_HDFS=1
++ ENABLE_HDFS_PERMISSIONS=false
++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
++ HADOOP_CONF_DIR=/etc/hadoop/conf
++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
++ HDFS_MASTER_MEMORY_FRACTION=0.4
++ NODEMANAGER_MEMORY_FRACTION=0.8
++ NUM_WORKERS=10
++ WORKERS=()
++ CORES_PER_MAP_TASK=1.0
++ CORES_PER_REDUCE_TASK=2.0
++ CORES_PER_APP_MASTER=2.0
++ HDFS_DATA_DIRS_PERM=700
++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
++ SPARK_CONF_DIR=/etc/spark/conf
++ SPARK_WORKER_MEMORY_FRACTION=0.8
++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
++ SPARK_DAEMON_MEMORY_FRACTION=0.15
++ SPARK_EXECUTORS_PER_VM=2
++ TEZ_CONF_DIR=/etc/tez/conf
++ TEZ_LIB_DIR=/usr/lib/tez
+ source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
++ is_centos
+++ . /etc/os-release
++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
++++ NAME='Debian GNU/Linux'
++++ VERSION_ID=10
++++ VERSION='10 (buster)'
++++ VERSION_CODENAME=buster
++++ ID=debian
++++ HOME_URL=https://www.debian.org/
++++ SUPPORT_URL=https://www.debian.org/support
++++ BUG_REPORT_URL=https://bugs.debian.org/
+++ echo debian
++ [[ debian == \c\e\n\t\o\s ]]
++ return 1
++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
+++ APT_SENTINEL=apt.lastupdate
++ readonly EXIT_CODE_INTERNAL_ERROR=1
++ EXIT_CODE_INTERNAL_ERROR=1
++ readonly EXIT_CODE_CLIENT_ERROR=2
++ EXIT_CODE_CLIENT_ERROR=2
+ source /usr/local/share/google/dataproc/bdutil/cluster_properties.sh
+ source /usr/local/share/google/dataproc/bdutil/components/components-helpers.sh
+ source /usr/local/share/google/dataproc/bdutil/components/startup-script-components.sh
+ source /usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
++ set -aeuxo pipefail
++ COMPONENTS_TO_ACTIVATE='hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn'
++ HDFS_ENABLED=true
++ ROLE=Master
++ MASTER_INDEX=0
++ set +a
+ run_with_logger --tag google-dataproc-startup
+ local tag=
+ local pid=10680
+ [[ --tag == \-\-\t\a\g ]]
+ tag=google-dataproc-startup
+ shift 2
+ [[ 0 -eq 0 ]]
+ exec
++ logger -s -t 'google-dataproc-startup[10680]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=()
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=()
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + cd /tmp
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + trap logstacktrace ERR
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Starting Dataproc post-hdfs startup script'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Starting Dataproc post-hdfs startup script'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Starting Dataproc post-hdfs startup script
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + COMPONENTS_TO_ACTIVATE_ARRAY=(${COMPONENTS_TO_ACTIVATE})
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_components hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql spark yarn
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + components=("$@")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local components
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + mkdir -p /tmp/dataproc/components/post-hdfs
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + for component in "${components[@]}"
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Activating post-hdfs component hdfs'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Activating post-hdfs component hdfs'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Activating post-hdfs component hdfs
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_in_background --tag post-hdfs-activate-component-hdfs post_hdfs_activate_component hdfs
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local -r pid=10688
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Started background process [post_hdfs_activate_component hdfs] as pid 10688'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Started background process [post_hdfs_activate_component hdfs] as pid 10688
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + for component in "${components[@]}"
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Activating post-hdfs component hive-metastore'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Activating post-hdfs component hive-metastore'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Activating post-hdfs component hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_in_background --tag post-hdfs-activate-component-hive-metastore post_hdfs_activate_component hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local -r pid=10689
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Started background process [post_hdfs_activate_component hive-metastore] as pid 10689'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Started background process [post_hdfs_activate_component hive-metastore] as pid 10689
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + for component in "${components[@]}"
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Activating post-hdfs component hive-server2'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Activating post-hdfs component hive-server2'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Activating post-hdfs component hive-server2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_in_background --tag post-hdfs-activate-component-hive-server2 post_hdfs_activate_component hive-server2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local -r pid=10690
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Started background process [post_hdfs_activate_component hive-server2] as pid 10690'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Started background process [post_hdfs_activate_component hive-server2] as pid 10690
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + for component in "${components[@]}"
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Activating post-hdfs component mapreduce'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Activating post-hdfs component mapreduce'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Activating post-hdfs component mapreduce
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_in_background --tag post-hdfs-activate-component-mapreduce post_hdfs_activate_component mapreduce
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local -r pid=10691
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Started background process [post_hdfs_activate_component mapreduce] as pid 10691'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Started background process [post_hdfs_activate_component mapreduce] as pid 10691
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + for component in "${components[@]}"
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Activating post-hdfs component miniconda3'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Activating post-hdfs component miniconda3'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Activating post-hdfs component miniconda3
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_in_background --tag post-hdfs-activate-component-miniconda3 post_hdfs_activate_component miniconda3
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local -r pid=10692
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Started background process [post_hdfs_activate_component miniconda3] as pid 10692'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Started background process [post_hdfs_activate_component miniconda3] as pid 10692
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + for component in "${components[@]}"
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Activating post-hdfs component mysql'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Activating post-hdfs component mysql'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Activating post-hdfs component mysql
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_in_background --tag post-hdfs-activate-component-mysql post_hdfs_activate_component mysql
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local -r pid=10693
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Started background process [post_hdfs_activate_component mysql] as pid 10693'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Started background process [post_hdfs_activate_component mysql] as pid 10693
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + for component in "${components[@]}"
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Activating post-hdfs component spark'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Activating post-hdfs component spark'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Activating post-hdfs component spark
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_in_background --tag post-hdfs-activate-component-spark post_hdfs_activate_component spark
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local -r pid=10694
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Started background process [post_hdfs_activate_component spark] as pid 10694'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Started background process [post_hdfs_activate_component spark] as pid 10694
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + for component in "${components[@]}"
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Activating post-hdfs component yarn'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Activating post-hdfs component yarn'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Activating post-hdfs component yarn
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_in_background --tag post-hdfs-activate-component-yarn post_hdfs_activate_component yarn
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local -r pid=10695
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=("${pid}" "${BACKGROUND_PROCESSES[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=("$*" "${BACKGROUND_COMMANDS[@]}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Started background process [post_hdfs_activate_component yarn] as pid 10695'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Started background process [post_hdfs_activate_component yarn] as pid 10695
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + wait_on_async_processes
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Waiting on async processes'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Waiting on async processes'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Waiting on async processes
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + (( i = 0 ))
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10695
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local 'cmd=post_hdfs_activate_component yarn'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Waiting on pid=10695 cmd=[post_hdfs_activate_component yarn]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Waiting on pid=10695 cmd=[post_hdfs_activate_component yarn]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Waiting on pid=10695 cmd=[post_hdfs_activate_component yarn]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'post_hdfs_activate_component yarn'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local status=0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + wait 10695
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_with_logger --tag post-hdfs-activate-component-yarn post_hdfs_activate_component yarn
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local tag=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10695
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tag=post-hdfs-activate-component-yarn
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ 2 -eq 0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_component yarn
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_with_logger --tag post-hdfs-activate-component-mysql post_hdfs_activate_component mysql
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local tag=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10693
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tag=post-hdfs-activate-component-mysql
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ 2 -eq 0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_component mysql
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_with_logger --tag post-hdfs-activate-component-hive-server2 post_hdfs_activate_component hive-server2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local tag=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10690
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tag=post-hdfs-activate-component-hive-server2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ 2 -eq 0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_component hive-server2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_with_logger --tag post-hdfs-activate-component-miniconda3 post_hdfs_activate_component miniconda3
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local tag=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10692
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tag=post-hdfs-activate-component-miniconda3
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ 2 -eq 0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_component miniconda3
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_with_logger --tag post-hdfs-activate-component-hdfs post_hdfs_activate_component hdfs
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local tag=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10688
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tag=post-hdfs-activate-component-hdfs
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ 2 -eq 0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_component hdfs
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_with_logger --tag post-hdfs-activate-component-spark post_hdfs_activate_component spark
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local tag=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10694
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tag=post-hdfs-activate-component-spark
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ 2 -eq 0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_component spark
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: ++ logger -s -t 'post-hdfs-activate-component-yarn[10695]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: ++ logger -s -t 'post-hdfs-activate-component-hive-server2[10690]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: ++ logger -s -t 'post-hdfs-activate-component-hdfs[10688]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: ++ logger -s -t 'post-hdfs-activate-component-spark[10694]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + (( status != 0 ))
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_with_logger --tag post-hdfs-activate-component-mapreduce post_hdfs_activate_component mapreduce
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local tag=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10691
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tag=post-hdfs-activate-component-mapreduce
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ 2 -eq 0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_component mapreduce
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: ++ logger -s -t 'post-hdfs-activate-component-miniconda3[10692]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + local -r component=hive-server2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + echo 'Running component activate post-hdfs script: /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: Running component activate post-hdfs script: /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + touch /tmp/dataproc/components/post-hdfs/hive-server2.running
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + local exit_code=0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + local -r component=spark
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + echo 'Running component activate post-hdfs script: /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: Running component activate post-hdfs script: /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + touch /tmp/dataproc/components/post-hdfs/spark.running
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + local exit_code=0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Command cmd=[post_hdfs_activate_component yarn] pid=10695 exited with 0'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tee /tmp/dataproc/commands/10695.done
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Command cmd=[post_hdfs_activate_component yarn] pid=10695 exited with 0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + (( ++i ))
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10694
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local 'cmd=post_hdfs_activate_component spark'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + loginfo 'Waiting on pid=10694 cmd=[post_hdfs_activate_component spark]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'Waiting on pid=10694 cmd=[post_hdfs_activate_component spark]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: Waiting on pid=10694 cmd=[post_hdfs_activate_component spark]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + echo 'post_hdfs_activate_component spark'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local status=0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + wait 10694
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: ++ logger -s -t 'post-hdfs-activate-component-mapreduce[10691]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + run_with_logger --tag post-hdfs-activate-component-hive-metastore post_hdfs_activate_component hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local tag=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + local pid=10689
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ --tag == \-\-\t\a\g ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + tag=post-hdfs-activate-component-hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + shift 2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + [[ 2 -eq 0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: + post_hdfs_activate_component hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-yarn[10695]: + local -r component=yarn
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-yarn[10695]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/yarn.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-yarn[10695]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/yarn.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-yarn[10695]: + echo 'Component yarn doesn'\''t have a post-hdfs script'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-yarn[10695]: Component yarn doesn't have a post-hdfs script
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-miniconda3[10692]: + local -r component=miniconda3
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-miniconda3[10692]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/miniconda3.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-miniconda3[10692]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/miniconda3.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-miniconda3[10692]: + echo 'Component miniconda3 doesn'\''t have a post-hdfs script'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-miniconda3[10692]: Component miniconda3 doesn't have a post-hdfs script
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: ++ logger -s -t 'post-hdfs-activate-component-mysql[10693]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hdfs[10688]: + local -r component=hdfs
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hdfs[10688]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/hdfs.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hdfs[10688]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hdfs.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hdfs[10688]: + echo 'Component hdfs doesn'\''t have a post-hdfs script'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hdfs[10688]: Component hdfs doesn't have a post-hdfs script
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: ++ logger -s -t 'post-hdfs-activate-component-hive-metastore[10689]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + set -euxo pipefail
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + set -euxo pipefail
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mapreduce[10691]: + local -r component=mapreduce
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mapreduce[10691]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/mapreduce.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mapreduce[10691]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/mapreduce.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mapreduce[10691]: + echo 'Component mapreduce doesn'\''t have a post-hdfs script'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mapreduce[10691]: Component mapreduce doesn't have a post-hdfs script
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mysql[10693]: + local -r component=mysql
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-metastore[10689]: + local -r component=hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mysql[10693]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/mysql.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../bdutil_env.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mysql[10693]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/mysql.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mysql[10693]: + echo 'Component mysql doesn'\''t have a post-hdfs script'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-mysql[10693]: Component mysql doesn't have a post-hdfs script
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ ENABLE_HDFS=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ NUM_WORKERS=10
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ WORKERS=()
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../bdutil_env.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ COMPONENTS_TO_ACTIVATE_ENV=/usr/local/share/google/dataproc/bdutil/components/components_to_activate_env.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ INSTALL_GCS_CONNECTOR=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ ENABLE_HDFS=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ ENABLE_HDFS_PERMISSIONS=false
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ NUM_WORKERS=10
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ WORKERS=()
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ CORES_PER_MAP_TASK=1.0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ CORES_PER_APP_MASTER=2.0
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ HDFS_DATA_DIRS_PERM=700
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ [[ -d /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-metastore[10689]: + local -r post_hdfs_script=/usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-metastore.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-metastore[10689]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-metastore.sh ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-metastore[10689]: + echo 'Component hive-metastore doesn'\''t have a post-hdfs script'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-metastore[10689]: Component hive-metastore doesn't have a post-hdfs script
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../bdutil_helpers.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ is_centos
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../bdutil_helpers.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ is_centos
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: +++ . /etc/os-release
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ NAME='Debian GNU/Linux'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ VERSION_ID=10
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ VERSION='10 (buster)'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ VERSION_CODENAME=buster
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ ID=debian
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ . /etc/os-release
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ PRETTY_NAME='Debian GNU/Linux 10 (buster)'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ NAME='Debian GNU/Linux'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ VERSION_ID=10
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ VERSION='10 (buster)'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ VERSION_CODENAME=buster
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ ID=debian
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ echo debian
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ HOME_URL=https://www.debian.org/
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ SUPPORT_URL=https://www.debian.org/support
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++++ BUG_REPORT_URL=https://bugs.debian.org/
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: +++ echo debian
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ return 1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ [[ debian == \c\e\n\t\o\s ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ return 1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: +++ APT_SENTINEL=apt.lastupdate
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../cluster_properties.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../../cluster_properties.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/spark.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/post-hdfs/hive-server2.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../shared/hive.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + [[ true == \t\r\u\e ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + start_hive_server2
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + wait_for_hive_metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + local timeout
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + source /usr/local/share/google/dataproc/bdutil/components/post-hdfs/../shared/spark.sh
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + set_log_tag post-hdfs-component-spark
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + local -r tag=post-hdfs-component-spark
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: + exec
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hive-metastore 300
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ local -r default_value=300
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ local actual_value
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: ++ logger -s -t 'post-hdfs-component-spark[10728]'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + [[ true == \t\r\u\e ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + [[ Master == \M\a\s\t\e\r ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + [[ 0 == \0 ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + start_spark_history_server
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + enable_service spark-history-server
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + local -r service=spark-history-server
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + local -r unit=spark-history-server.service
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + retry_constant_short systemctl enable spark-history-server.service
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + retry_constant_custom 30 1 systemctl enable spark-history-server.service
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ get_dataproc_property startup.component.service-binding-timeout.hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + local -r max_retry_time=30
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + local -r retry_delay=1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + cmd=("${@:3}")
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + local -r cmd
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + local -r max_retries=30
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: + set +x
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: About to run 'systemctl enable spark-history-server.service' with retries...
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: spark-history-server.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:03 post-hdfs-component-spark[10728]: Executing: /lib/systemd/systemd-sysv-install enable spark-history-server
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ local property_value
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ get_java_property /etc/google-dataproc/dataproc.properties startup.component.service-binding-timeout.hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ local -r property_name=startup.component.service-binding-timeout.hive-metastore
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ local property_value
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++++ cut -d = -f 2-
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++++ grep '^startup.component.service-binding-timeout.hive-metastore=' /etc/google-dataproc/dataproc.properties
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++++ tail -n 1
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ property_value=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++++ echo ''
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ property_value=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ echo ''
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ actual_value=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ [[ -n '' ]]
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ echo 300
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + timeout=300
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: + local metastore_uris
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.uris
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ local -r path=/etc/hive/conf/hive-site.xml
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ local -r property=hive.metastore.uris
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ local -r default_value=
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: ++ local val
<13>Mar 22 23:36:03 google-dataproc-startup[10680]: <13>Mar 22 23:36:03 post-hdfs-activate-component-hive-server2[10690]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: ++ val=thrift://student-03-c0c0225f5598-qwiklab-m:9083
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: ++ [[ thrift://student-03-c0c0225f5598-qwiklab-m:9083 == \N\o\n\e ]]
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: ++ echo thrift://student-03-c0c0225f5598-qwiklab-m:9083
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + metastore_uris=thrift://student-03-c0c0225f5598-qwiklab-m:9083
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local host
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: ++ echo thrift://student-03-c0c0225f5598-qwiklab-m:9083
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: ++ sed -n 's#.*://\(.*\):.*#\1#p'
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + host=student-03-c0c0225f5598-qwiklab-m
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + [[ -z student-03-c0c0225f5598-qwiklab-m ]]
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local port
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: ++ sed -n 's#.*://.*:\(.*\)#\1#p'
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: ++ echo thrift://student-03-c0c0225f5598-qwiklab-m:9083
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + port=9083
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + [[ -z 9083 ]]
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + wait_for_port hive-metastore student-03-c0c0225f5598-qwiklab-m 9083 300
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r name=hive-metastore
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r host=student-03-c0c0225f5598-qwiklab-m
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r port=9083
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r timeout=300
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r capped_timeout=300
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + loginfo 'Waiting 300 seconds for service to come up on host=student-03-c0c0225f5598-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + echo 'Waiting 300 seconds for service to come up on host=student-03-c0c0225f5598-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: Waiting 300 seconds for service to come up on host=student-03-c0c0225f5598-qwiklab-m port=9083 name=hive-metastore.
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + retry_constant_custom 300 1 nc -v -z -w 1 student-03-c0c0225f5598-qwiklab-m 9083
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r max_retry_time=300
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r retry_delay=1
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + cmd=("${@:3}")
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r cmd
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r max_retries=300
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + set +x
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: About to run 'nc -v -z -w 1 student-03-c0c0225f5598-qwiklab-m 9083' with retries...
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: Connection to student-03-c0c0225f5598-qwiklab-m 9083 port [tcp/*] succeeded!
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: nc -v -z -w 1 student-03-c0c0225f5598-qwiklab-m 9083 succeeded.
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + return 0
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + loginfo 'Service up on host=student-03-c0c0225f5598-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + echo 'Service up on host=student-03-c0c0225f5598-qwiklab-m port=9083 name=hive-metastore.'
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: Service up on host=student-03-c0c0225f5598-qwiklab-m port=9083 name=hive-metastore.
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + enable_service hive-server2
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r service=hive-server2
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r unit=hive-server2.service
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + retry_constant_short systemctl enable hive-server2.service
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + retry_constant_custom 30 1 systemctl enable hive-server2.service
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r max_retry_time=30
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r retry_delay=1
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + cmd=("${@:3}")
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r cmd
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + local -r max_retries=30
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: + set +x
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: About to run 'systemctl enable hive-server2.service' with retries...
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: hive-server2.service is not a native service, redirecting to systemd-sysv-install.
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-hive-server2[10690]: Executing: /lib/systemd/systemd-sysv-install enable hive-server2
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: systemctl enable spark-history-server.service succeeded.
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: + return 0
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: + local -r drop_in_dir=/etc/systemd/system/spark-history-server.service.d
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: + mkdir -p /etc/systemd/system/spark-history-server.service.d
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: + local props
<13>Mar 22 23:36:04 google-dataproc-startup[10680]: <13>Mar 22 23:36:04 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:04 post-hdfs-component-spark[10728]: ++ systemctl show spark-history-server.service -p Restart,RemainAfterExit
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: systemctl enable hive-server2.service succeeded.
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + return 0
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local -r drop_in_dir=/etc/systemd/system/hive-server2.service.d
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + mkdir -p /etc/systemd/system/hive-server2.service.d
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + props='Restart=no
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: RemainAfterExit=no'
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + [[ spark-history-server != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + [[ spark-history-server != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + [[ Restart=no
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + [[ Restart=no
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/spark-history-server.service.d
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local props
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: ++ systemctl show hive-server2.service -p Restart,RemainAfterExit
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + [[ spark-history-server == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + [[ spark-history-server == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + retry_constant systemctl start spark-history-server
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + retry_constant_custom 300 1 systemctl start spark-history-server
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + local -r max_retry_time=300
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + local -r retry_delay=1
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + cmd=("${@:3}")
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + local -r cmd
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + local -r max_retries=300
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: + set +x
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: About to run 'systemctl start spark-history-server' with retries...
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + props='Restart=no
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: RemainAfterExit=no'
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ hive-server2 != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ hive-server2 != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ Restart=no
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ Restart=no
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: RemainAfterExit=no == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + ln -s -f /etc/systemd/system/common/restart.conf /etc/systemd/system/hive-server2.service.d
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ hive-server2 == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + [[ hive-server2 == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + retry_constant systemctl start hive-server2
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + retry_constant_custom 300 1 systemctl start hive-server2
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local -r max_retry_time=300
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local -r retry_delay=1
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + cmd=("${@:3}")
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local -r cmd
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + local -r max_retries=300
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: + set +x
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: About to run 'systemctl start hive-server2' with retries...
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:05 post-hdfs-component-spark[10728]: Warning: The unit file, source configuration file or drop-ins of spark-history-server.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Mar 22 23:36:05 google-dataproc-startup[10680]: <13>Mar 22 23:36:05 post-hdfs-activate-component-hive-server2[10690]: Warning: The unit file, source configuration file or drop-ins of hive-server2.service changed on disk. Run 'systemctl daemon-reload' to reload units.
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: systemctl start hive-server2 succeeded.
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + return 0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local thrift_port
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.server2.thrift.port 10000
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ local -r path=/etc/hive/conf/hive-site.xml
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ local -r property=hive.server2.thrift.port
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ local -r default_value=10000
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ local val
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++ bdconfig get_property_value --configuration_file /etc/hive/conf/hive-site.xml --name hive.server2.thrift.port
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:08 post-hdfs-component-spark[10728]: systemctl start spark-history-server succeeded.
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-spark[10694]: <13>Mar 22 23:36:08 post-hdfs-component-spark[10728]: + return 0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-spark[10694]: + [[ 0 -ne 0 ]]
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-spark[10694]: + touch /tmp/dataproc/components/post-hdfs/spark.done
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( status != 0 ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + tee /tmp/dataproc/commands/10694.done
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'Command cmd=[post_hdfs_activate_component spark] pid=10694 exited with 0'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: Command cmd=[post_hdfs_activate_component spark] pid=10694 exited with 0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( ++i ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local pid=10693
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local 'cmd=post_hdfs_activate_component mysql'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + loginfo 'Waiting on pid=10693 cmd=[post_hdfs_activate_component mysql]'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'Waiting on pid=10693 cmd=[post_hdfs_activate_component mysql]'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: Waiting on pid=10693 cmd=[post_hdfs_activate_component mysql]
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'post_hdfs_activate_component mysql'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local status=0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + wait 10693
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( status != 0 ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + tee /tmp/dataproc/commands/10693.done
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'Command cmd=[post_hdfs_activate_component mysql] pid=10693 exited with 0'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: Command cmd=[post_hdfs_activate_component mysql] pid=10693 exited with 0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( ++i ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local pid=10692
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local 'cmd=post_hdfs_activate_component miniconda3'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + loginfo 'Waiting on pid=10692 cmd=[post_hdfs_activate_component miniconda3]'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'Waiting on pid=10692 cmd=[post_hdfs_activate_component miniconda3]'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: Waiting on pid=10692 cmd=[post_hdfs_activate_component miniconda3]
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'post_hdfs_activate_component miniconda3'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local status=0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + wait 10692
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( status != 0 ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + tee /tmp/dataproc/commands/10692.done
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'Command cmd=[post_hdfs_activate_component miniconda3] pid=10692 exited with 0'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: Command cmd=[post_hdfs_activate_component miniconda3] pid=10692 exited with 0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( ++i ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local pid=10691
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local 'cmd=post_hdfs_activate_component mapreduce'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + loginfo 'Waiting on pid=10691 cmd=[post_hdfs_activate_component mapreduce]'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'Waiting on pid=10691 cmd=[post_hdfs_activate_component mapreduce]'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: Waiting on pid=10691 cmd=[post_hdfs_activate_component mapreduce]
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'post_hdfs_activate_component mapreduce'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local status=0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + wait 10691
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( status != 0 ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + tee /tmp/dataproc/commands/10691.done
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'Command cmd=[post_hdfs_activate_component mapreduce] pid=10691 exited with 0'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: Command cmd=[post_hdfs_activate_component mapreduce] pid=10691 exited with 0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( ++i ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local pid=10690
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local 'cmd=post_hdfs_activate_component hive-server2'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + loginfo 'Waiting on pid=10690 cmd=[post_hdfs_activate_component hive-server2]'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'Waiting on pid=10690 cmd=[post_hdfs_activate_component hive-server2]'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: Waiting on pid=10690 cmd=[post_hdfs_activate_component hive-server2]
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + echo 'post_hdfs_activate_component hive-server2'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + local status=0
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: + wait 10690
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ val=None
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ [[ None == \N\o\n\e ]]
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ val=10000
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ echo 10000
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + thrift_port=10000
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local timeout
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ get_dataproc_property_or_default startup.component.service-binding-timeout.hive-server2 300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ local -r property_name=startup.component.service-binding-timeout.hive-server2
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ local -r default_value=300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ local actual_value
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++ get_dataproc_property startup.component.service-binding-timeout.hive-server2
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++ local -r property_name=startup.component.service-binding-timeout.hive-server2
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++ local property_value
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++++ get_java_property /etc/google-dataproc/dataproc.properties startup.component.service-binding-timeout.hive-server2
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++++ local -r property_file=/etc/google-dataproc/dataproc.properties
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++++ local -r property_name=startup.component.service-binding-timeout.hive-server2
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++++ local property_value
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++++ sed -r 's/\\([#!=:])/\1/g'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++++ grep '^startup.component.service-binding-timeout.hive-server2=' /etc/google-dataproc/dataproc.properties
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++++ tail -n 1
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++++ cut -d = -f 2-
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++++ property_value=
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++++ echo ''
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++ property_value=
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: +++ echo ''
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ actual_value=
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ [[ -n '' ]]
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: ++ echo 300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + timeout=300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + wait_for_port hive-server2 student-03-c0c0225f5598-qwiklab-m 10000 300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r name=hive-server2
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r host=student-03-c0c0225f5598-qwiklab-m
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r port=10000
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r timeout=300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r capped_timeout=300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + loginfo 'Waiting 300 seconds for service to come up on host=student-03-c0c0225f5598-qwiklab-m port=10000 name=hive-server2.'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + echo 'Waiting 300 seconds for service to come up on host=student-03-c0c0225f5598-qwiklab-m port=10000 name=hive-server2.'
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: Waiting 300 seconds for service to come up on host=student-03-c0c0225f5598-qwiklab-m port=10000 name=hive-server2.
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + retry_constant_custom 300 1 nc -v -z -w 1 student-03-c0c0225f5598-qwiklab-m 10000
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r max_retry_time=300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r retry_delay=1
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + cmd=("${@:3}")
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r cmd
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + local -r max_retries=300
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: + set +x
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: About to run 'nc -v -z -w 1 student-03-c0c0225f5598-qwiklab-m 10000' with retries...
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:08 google-dataproc-startup[10680]: <13>Mar 22 23:36:08 post-hdfs-activate-component-hive-server2[10690]: 'nc -v -z -w 1 student-03-c0c0225f5598-qwiklab-m 10000' attempt 1 failed! Sleeping 1s.
<13>Mar 22 23:36:09 google-dataproc-startup[10680]: <13>Mar 22 23:36:09 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:10 google-dataproc-startup[10680]: <13>Mar 22 23:36:10 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:12 google-dataproc-startup[10680]: <13>Mar 22 23:36:12 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:13 google-dataproc-startup[10680]: <13>Mar 22 23:36:13 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:14 google-dataproc-startup[10680]: <13>Mar 22 23:36:14 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:15 google-dataproc-startup[10680]: <13>Mar 22 23:36:15 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:16 google-dataproc-startup[10680]: <13>Mar 22 23:36:16 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:17 google-dataproc-startup[10680]: <13>Mar 22 23:36:17 post-hdfs-activate-component-hive-server2[10690]: nc: connect to student-03-c0c0225f5598-qwiklab-m port 10000 (tcp) failed: Connection refused
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: <13>Mar 22 23:36:18 post-hdfs-activate-component-hive-server2[10690]: Connection to student-03-c0c0225f5598-qwiklab-m 10000 port [tcp/webmin] succeeded!
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: <13>Mar 22 23:36:18 post-hdfs-activate-component-hive-server2[10690]: nc -v -z -w 1 student-03-c0c0225f5598-qwiklab-m 10000 succeeded.
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: <13>Mar 22 23:36:18 post-hdfs-activate-component-hive-server2[10690]: + return 0
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: <13>Mar 22 23:36:18 post-hdfs-activate-component-hive-server2[10690]: + loginfo 'Service up on host=student-03-c0c0225f5598-qwiklab-m port=10000 name=hive-server2.'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: <13>Mar 22 23:36:18 post-hdfs-activate-component-hive-server2[10690]: + echo 'Service up on host=student-03-c0c0225f5598-qwiklab-m port=10000 name=hive-server2.'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: <13>Mar 22 23:36:18 post-hdfs-activate-component-hive-server2[10690]: Service up on host=student-03-c0c0225f5598-qwiklab-m port=10000 name=hive-server2.
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: <13>Mar 22 23:36:18 post-hdfs-activate-component-hive-server2[10690]: + [[ 0 -ne 0 ]]
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: <13>Mar 22 23:36:18 post-hdfs-activate-component-hive-server2[10690]: + touch /tmp/dataproc/components/post-hdfs/hive-server2.done
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( status != 0 ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + tee /tmp/dataproc/commands/10690.done
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + echo 'Command cmd=[post_hdfs_activate_component hive-server2] pid=10690 exited with 0'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: Command cmd=[post_hdfs_activate_component hive-server2] pid=10690 exited with 0
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( ++i ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + local pid=10689
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + local 'cmd=post_hdfs_activate_component hive-metastore'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + loginfo 'Waiting on pid=10689 cmd=[post_hdfs_activate_component hive-metastore]'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + echo 'Waiting on pid=10689 cmd=[post_hdfs_activate_component hive-metastore]'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: Waiting on pid=10689 cmd=[post_hdfs_activate_component hive-metastore]
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + echo 'post_hdfs_activate_component hive-metastore'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + local status=0
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + wait 10689
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( status != 0 ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + tee /tmp/dataproc/commands/10689.done
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + echo 'Command cmd=[post_hdfs_activate_component hive-metastore] pid=10689 exited with 0'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: Command cmd=[post_hdfs_activate_component hive-metastore] pid=10689 exited with 0
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( ++i ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + local pid=10688
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + local 'cmd=post_hdfs_activate_component hdfs'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + loginfo 'Waiting on pid=10688 cmd=[post_hdfs_activate_component hdfs]'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + echo 'Waiting on pid=10688 cmd=[post_hdfs_activate_component hdfs]'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: Waiting on pid=10688 cmd=[post_hdfs_activate_component hdfs]
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + echo 'post_hdfs_activate_component hdfs'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + local status=0
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + wait 10688
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( status != 0 ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + tee /tmp/dataproc/commands/10688.done
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + echo 'Command cmd=[post_hdfs_activate_component hdfs] pid=10688 exited with 0'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: Command cmd=[post_hdfs_activate_component hdfs] pid=10688 exited with 0
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( ++i ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + (( i < 8 ))
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + BACKGROUND_PROCESSES=()
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + BACKGROUND_COMMANDS=()
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + loginfo 'All done'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: + echo 'All done'
<13>Mar 22 23:36:18 google-dataproc-startup[10680]: All done
